# Neural Rink - PPO Training Configuration
# Unity ML-Agents 4.0 configuration for goalie agent training

behaviors:
  GoalieAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 64
      buffer_size: 2048
      learning_rate: 3.0e-4
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: constant
      epsilon_schedule: linear
      lambd_schedule: constant
      
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple
      memory: null
      goal_conditioning_type: hyper
      
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        
    keep_checkpoints: 5
    max_steps: 1000000
    time_horizon: 64
    summary_freq: 10000
    
    # Environment-specific settings
    threaded: false
    max_lifetime_reward: 1000
    reward_buff_cap: 1.0
    
    # Deterministic training settings
    deterministic: true
    
    # Checkpoint settings
    checkpoint_interval: 50000
    
    # Training environment settings
    env_settings:
      env_path: null
      num_envs: 1
      base_port: 5005
      num_areas: 1
      timeout_wait: 60
      
# Global settings
default_settings:
  trainer_type: ppo
  max_steps: 1000000
  
# Environment configuration
environment_parameters:
  episode_length:
    default: 30
    description: "Maximum episode length in seconds"
    
  randomize_physics:
    default: false
    description: "Whether to randomize physics parameters during training"
    
  physics_randomization_range:
    default: 0.1
    description: "Range for physics parameter randomization"
    
  save_bonus:
    default: 1.0
    description: "Reward bonus for successful saves"
    
  goal_penalty:
    default: -1.0
    description: "Penalty for goals conceded"
    
  distance_penalty:
    default: -0.01
    description: "Penalty for distance from goal center"
    
  time_penalty:
    default: -0.001
    description: "Penalty for episode time elapsed"

# Curriculum learning (optional)
curriculum:
  measure: success_rate
  thresholds: [0.1, 0.3, 0.5, 0.7, 0.9]
  min_lesson_length: 100
  signal_smoothing: true
  
  # Lesson 1: Basic positioning
  lesson_1:
    episode_length: 15
    save_bonus: 2.0
    goal_penalty: -0.5
    
  # Lesson 2: Movement and timing
  lesson_2:
    episode_length: 25
    save_bonus: 1.5
    goal_penalty: -1.0
    distance_penalty: -0.005
    
  # Lesson 3: Advanced positioning
  lesson_3:
    episode_length: 30
    save_bonus: 1.0
    goal_penalty: -1.0
    distance_penalty: -0.01
    
  # Lesson 4: Full difficulty
  lesson_4:
    episode_length: 30
    save_bonus: 1.0
    goal_penalty: -1.0
    distance_penalty: -0.01
    time_penalty: -0.001
    
  # Lesson 5: Randomized physics
  lesson_5:
    episode_length: 30
    save_bonus: 1.0
    goal_penalty: -1.0
    distance_penalty: -0.01
    time_penalty: -0.001
    randomize_physics: true
    physics_randomization_range: 0.15

# Training monitoring
monitor:
  enabled: true
  log_dir: "neural_rink_logs"
  tensorboard: true
  wandb: false
  
# Checkpoint management
checkpoint:
  enabled: true
  interval: 50000
  max_checkpoints: 10
  save_best: true
  
# Performance optimization
performance:
  gpu_memory_growth: true
  mixed_precision: false
  deterministic: true
  
# Debugging and logging
debug:
  log_level: INFO
  log_interval: 1000
  save_models: true
  save_replay_buffer: false
